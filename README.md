запуск rackup
curl -H "Content-Type: application/json" -X PUT -d '{"rate":"2"}' http://localhost:9292/api/v1/posts/567
curl "http://localhost:9292/api/v1/posts?rating=3.5&limit=20"
curl -H "Content-Type: application/json" -d '{"title":"post_title", "content":"post_content", "user_login":"user1", "user_ip":"192.168.1.100"}' http://localhost:9292/api/v1/posts/new

curl -w "%{time_total}\n" -X PUT -d 'rate=2' http://localhost:9292/api/v1/posts/567





# Техническое задание

Требуется создать JSON API сервис на Ruby. В качестве веб-фреймворка, можете использовать sinatra, hanami, roda или что-нибудь другое, но не Ruby on Rails. Доспут к БД можете осуществлять с помощью ORM (active_record, sequel, rom), можете и без ORM, как посчитаете нужным.

## Сущности:
·    Юзер. Имеет только логин.
·    Пост. Принадлежит юзеру. Имеет заголовок, содержание, айпи автора (сохраняется отдельно для каждого поста).
·    Оценка. Принадлежит посту. Принимает значение от 1 до 5.

## Экшены:

·    Создать пост. Принимает заголовок и содержание поста (не могут быть пустыми), а также логин и айпи автора. Если автора с таким логином еще нет, необходимо его создать. Возвращает либо атрибуты поста со статусом 200, либо ошибки валидации со статусом 422.

·    Поставить оценку посту. Принимает айди поста и значение, возвращает новый средний рейтинг поста. Важно: экшен должен корректно отрабатывать при любом количестве конкурентных запросов на оценку одного и того же поста.

·    Получить топ N постов по среднему рейтингу. Просто массив объектов с заголовками и содержанием.

·    Получить список айпи, с которых постило несколько разных авторов. Массив объектов с полями: айпи и массив логинов авторов.

## База данных
Базу данных используем PostgreSQL. Для девелопмента написать скрипт в db/seeds.rb, который генерирует тестовые данные. Часть постов должна получить оценки. Скрипт должен использовать созданный JSON API сервер (можно посылать запросы курлом или еще чем-нибудь).

Постов в базе должно быть хотя бы 200к, авторов лучше сделать в районе 100 штук, айпишников использовать штук 50 разных.

Экшены должны на стандартном железе работать достаточно быстро как для указанного объема данных (быстрее 100 мс, если будет работать медленне, то ничего страшного, все равно присылайте решение), так и для намного большего, то есть нужен хороший запас в плане оптимизации запросов. Для этого можно использовать денормализацию данных и любые другие средства БД. Можно использовать любые нужные гемы, обязательно наличие спеков, хорошо покрывающих разные кейсы. Архитектуру сервиса организуйте на "свой вкус". Желательно не использовать генераторов и вообще обойтись без лишних мусорных файлов в репозитории.

